{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be61bf23",
   "metadata": {},
   "source": [
    "# Adversarial Bias Mitigation Experiments\n",
    "\n",
    "This notebook implements adversarial debiasing, modeled after the implementation in:\n",
    "\n",
    "[1] X. Han, T. Baldwin, and T. Cohn, “Towards Equal Opportunity Fairness through Adversarial Learning,” May 15, 2022, arXiv: arXiv:2203.06317. doi: 10.48550/arXiv.2203.06317.\n",
    "\n",
    "[2] Xudong Han, Timothy Baldwin, and Trevor Cohn. 2021. Diverse Adversaries for Mitigating Bias in Training. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 2760–2765, Online. Association for Computational Linguistics.\n",
    "\n",
    "https://github.com/INK-USC/Upstream-Bias-Mitigation and https://github.com/HanXudong/Diverse_Adversaries_for_Mitigating_Bias_in_Training\n",
    "\n",
    "## NOTES\n",
    "- Check what the models being saved in the src/models file are.\n",
    "- Experiment with multiple discriminators, as in [2] (There is currently an error in training multiples)\n",
    "- Add explicit equal opportunity criterion, as in [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31b1cac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory:  /Users/pia4/Desktop/BFH/github_repos/bias_mitigation_BERT_multilingual/src\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer, RobertaForSequenceClassification, RobertaTokenizer\n",
    "\n",
    "import os #,argparse,time\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Get directory of this script (works for any repository location)\n",
    "dir_path = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()\n",
    "# Ensure we're in the src directory\n",
    "if not dir_path.endswith('src'):\n",
    "    # If we're in a notebook, find the src directory relative to current location\n",
    "    current_dir = os.getcwd()\n",
    "    if 'src' in current_dir:\n",
    "        # Already in src or subdirectory\n",
    "        dir_path = current_dir\n",
    "    else:\n",
    "        # Look for src directory\n",
    "        potential_src = os.path.join(current_dir, 'src')\n",
    "        if os.path.exists(potential_src):\n",
    "            dir_path = potential_src\n",
    "        else:\n",
    "            print(f\"Warning: Could not find 'src' directory. Using current directory: {current_dir}\")\n",
    "            dir_path = current_dir\n",
    "\n",
    "os.chdir(dir_path)\n",
    "print(\"Current Directory: \", os.getcwd())\n",
    "\n",
    "from utils.dataloaders_BERT import BiosDataset, create_dataloaders\n",
    "from utils.model_config import load_model_and_tokenizer, model_types\n",
    "from utils.discriminator import Discriminator\n",
    "from scripts_adv_debias import train_and_evaluate, eval_main, train_leakage_discriminator, adv_eval_epoch\n",
    "\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from utils.customized_loss import DiffLoss\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "from utils.eval_metrices import group_evaluation, leakage_evaluation\n",
    "\n",
    "from pathlib import Path, PureWindowsPath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39f757e",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2383da91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "pretrained = True # Conduct experiments with pre-trained models, without fine-tuning (for baselines, adversarial = False) \n",
    "\n",
    "#### Experiment Settings ####\n",
    "adversarial = False # Change to True to enable adversarial training\n",
    "run_training = True # Train the model (if False, load existing model)\n",
    "run_occupation_bias = True # Conduct occupation prediction bias evaluation\n",
    "run_leakage_evaluation = True # Conduct gender prediction/leakage evaluation\n",
    "run_intrinsic = True # Conduct WEAT-measures on pretrained or finetuned model.\n",
    "balance_sensitive = True # Sample from dataset so that within each occupation category, sensitive attributes are balanced.\n",
    "use_last = False # Use main model from end of training instead of best validation performance.\n",
    "masked = True # Use data masking to remove sensitive information from text inputs.\n",
    "language = \"is\"  # Changed to Icelandic\n",
    "num_epochs = 15\n",
    "num_discriminators = 1\n",
    "\n",
    "#### DataLoader Parameters ####\n",
    "params = {'batch_size': 128,    \n",
    "        'shuffle': True,\n",
    "        'num_workers': 0}\n",
    "\n",
    "#### Model specs ####\n",
    "model_type = \"bert\"\n",
    "if language == \"is\":\n",
    "    model_type = \"roberta\"  # Use RoBERTa for Icelandic\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")           \n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "data_path = \"../data/bios/\"+f\"{language}/\"\n",
    "output_dir = f\"../results/adv_debias/{language}/\"\n",
    "\n",
    "\n",
    "#### Adversarial Training Arguments ####\n",
    "class Args:\n",
    "    num_epochs = num_epochs\n",
    "    use_fp16 = False\n",
    "    cuda = \"cuda\"\n",
    "    hidden_size = 768 # Change to match your model's hidden size\n",
    "    emb_size = 2304 # Change to match your model's embedding size\n",
    "    num_classes = 2 # Number of private labels for each discriminator\n",
    "    adv = adversarial \n",
    "    # adv_level = -1\n",
    "    lr = 0.00001\n",
    "    LAMBDA = 0.8 # Balance coefficient for discriminator loss\n",
    "    n_discriminator = num_discriminators\n",
    "    adv_units = 256\n",
    "    # ratio = 0.8\n",
    "    DL = True # Change to True to enable DiffLoss\n",
    "    diff_LAMBDA = 10**3.7 # Coefficient for differential loss\n",
    "    experiment_type = \"adv_debiasing\" if adversarial else \"standard\" if not pretrained else \"pretrained\"\n",
    "    # protected_attributes = protected_attributes\n",
    "    diff_loss = DiffLoss()\n",
    "    use_last = use_last\n",
    "    masked = masked\n",
    "    balanced = balance_sensitive\n",
    "    # data_path = #Your data path\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# Directory for saving the trained model\n",
    "output_model_dir = os.path.join(output_dir, f\"{model_type}_{args.experiment_type}_model\")\n",
    "\n",
    "# If adversarial training is used, add the number of discriminators to the output directory name\n",
    "if not pretrained:\n",
    "    if adversarial:\n",
    "        output_model_dir += f\"_{num_discriminators}discriminators\"\n",
    "    \n",
    "    # If balanced dataset, add to output directory name\n",
    "    if balance_sensitive:\n",
    "        output_model_dir += f\"_balanced\"\n",
    "    \n",
    "    # If masking, add to output directory name\n",
    "    if masked:\n",
    "        output_model_dir += f\"_masked\"\n",
    "    \n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0919bfdc",
   "metadata": {},
   "source": [
    "---\n",
    "## Load and prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2c23df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the created train and test splits\n",
    "language_code = language.upper()\n",
    "\n",
    "if not masked:\n",
    "    if balance_sensitive:\n",
    "        train_split_path = os.path.join(os.path.dirname(data_path), 'train_split_balanced.csv')\n",
    "        test_split_path = os.path.join(os.path.dirname(data_path), 'test_split_balanced.csv')\n",
    "        train_df = pd.read_csv(train_split_path)\n",
    "        test_df = pd.read_csv(test_split_path)\n",
    "    else:\n",
    "        train_split_path = os.path.join(os.path.dirname(data_path), 'train_split.csv')\n",
    "        test_split_path = os.path.join(os.path.dirname(data_path), 'test_split.csv')\n",
    "        train_df = pd.read_csv(train_split_path)\n",
    "        test_df = pd.read_csv(test_split_path)\n",
    "\n",
    "else:\n",
    "    if balance_sensitive:\n",
    "        train_split_path = os.path.join(os.path.dirname(data_path), 'train_split_balanced_masked.csv')\n",
    "        test_split_path = os.path.join(os.path.dirname(data_path), 'test_split_balanced_masked.csv')\n",
    "        train_df = pd.read_csv(train_split_path)\n",
    "        test_df = pd.read_csv(test_split_path)\n",
    "    else:\n",
    "        train_split_path = os.path.join(os.path.dirname(data_path), 'train_split_masked.csv')\n",
    "        test_split_path = os.path.join(os.path.dirname(data_path), 'test_split_masked.csv')\n",
    "        train_df = pd.read_csv(train_split_path)\n",
    "        test_df = pd.read_csv(test_split_path)\n",
    "    \n",
    "if language != \"en\":\n",
    "    if masked:\n",
    "        train_df['hard_text'] = train_df[f'hard_text_masked_{language_code}']\n",
    "        test_df['hard_text'] = test_df[f'hard_text_masked_{language_code}']\n",
    "    else:\n",
    "        train_df['hard_text'] = train_df[f'hard_text_{language_code}']\n",
    "        test_df['hard_text'] = test_df[f'hard_text_{language_code}']\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a129d02-ee06-4cbe-a32c-1b4a7b4b7067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>hard_text</th>\n",
       "      <th>profession</th>\n",
       "      <th>gender</th>\n",
       "      <th>hard_text_masked</th>\n",
       "      <th>hard_text_masked_IS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Þau eru vottuð sem Advanced Jivamukti og hafa ...</td>\n",
       "      <td>yoga_teacher</td>\n",
       "      <td>Male</td>\n",
       "      <td>they are Advanced Jivamukti Certified and a me...</td>\n",
       "      <td>Þau eru vottuð sem Advanced Jivamukti og hafa ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[NAME] er útskrifaður frá Háskólanum í Illinoi...</td>\n",
       "      <td>paralegal</td>\n",
       "      <td>Male</td>\n",
       "      <td>[NAME] is a graduate of the University of Illi...</td>\n",
       "      <td>[NAME] er útskrifaður frá Háskólanum í Illinoi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Þau misstu föður sinn aðeins 11 ára gömul og f...</td>\n",
       "      <td>poet</td>\n",
       "      <td>Male</td>\n",
       "      <td>Having lost their dad at just 11 years old, th...</td>\n",
       "      <td>Þau misstu föður sinn aðeins 11 ára gömul og f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Þeir koma með sinn sérstaka tegund af dökkum, ...</td>\n",
       "      <td>comedian</td>\n",
       "      <td>Male</td>\n",
       "      <td>they bring their particular brand of dark, irr...</td>\n",
       "      <td>Þeir koma með sinn sérstaka tegund af dökkum, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Þeir eru sérfræðingar í hollri fæðu, mat og næ...</td>\n",
       "      <td>dietitian</td>\n",
       "      <td>Male</td>\n",
       "      <td>they are an expert on healthy eating, food, an...</td>\n",
       "      <td>Þeir eru sérfræðingar í hollri fæðu, mat og næ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                          hard_text  \\\n",
       "0           0  Þau eru vottuð sem Advanced Jivamukti og hafa ...   \n",
       "1           1  [NAME] er útskrifaður frá Háskólanum í Illinoi...   \n",
       "2           2  Þau misstu föður sinn aðeins 11 ára gömul og f...   \n",
       "3           3  Þeir koma með sinn sérstaka tegund af dökkum, ...   \n",
       "4           4  Þeir eru sérfræðingar í hollri fæðu, mat og næ...   \n",
       "\n",
       "     profession gender                                   hard_text_masked  \\\n",
       "0  yoga_teacher   Male  they are Advanced Jivamukti Certified and a me...   \n",
       "1     paralegal   Male  [NAME] is a graduate of the University of Illi...   \n",
       "2          poet   Male  Having lost their dad at just 11 years old, th...   \n",
       "3      comedian   Male  they bring their particular brand of dark, irr...   \n",
       "4     dietitian   Male  they are an expert on healthy eating, food, an...   \n",
       "\n",
       "                                 hard_text_masked_IS  \n",
       "0  Þau eru vottuð sem Advanced Jivamukti og hafa ...  \n",
       "1  [NAME] er útskrifaður frá Háskólanum í Illinoi...  \n",
       "2  Þau misstu föður sinn aðeins 11 ára gömul og f...  \n",
       "3  Þeir koma með sinn sérstaka tegund af dökkum, ...  \n",
       "4  Þeir eru sérfræðingar í hollri fæðu, mat og næ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e2a2abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set distribution:\n",
      "profession         gender\n",
      "accountant         Female    80\n",
      "                   Male      80\n",
      "architect          Female    80\n",
      "                   Male      80\n",
      "attorney           Female    80\n",
      "                   Male      80\n",
      "chiropractor       Female    80\n",
      "                   Male      80\n",
      "comedian           Female    80\n",
      "                   Male      80\n",
      "composer           Female    80\n",
      "                   Male      80\n",
      "dentist            Female    80\n",
      "                   Male      80\n",
      "dietitian          Female    80\n",
      "                   Male      80\n",
      "dj                 Female    80\n",
      "                   Male      80\n",
      "filmmaker          Female    80\n",
      "                   Male      80\n",
      "interior_designer  Female    80\n",
      "                   Male      80\n",
      "journalist         Female    80\n",
      "                   Male      80\n",
      "model              Female    80\n",
      "                   Male      80\n",
      "nurse              Female    80\n",
      "                   Male      80\n",
      "painter            Female    80\n",
      "                   Male      80\n",
      "paralegal          Female    80\n",
      "                   Male      80\n",
      "pastor             Female    80\n",
      "                   Male      80\n",
      "personal_trainer   Female    80\n",
      "                   Male      80\n",
      "photographer       Female    80\n",
      "                   Male      80\n",
      "physician          Female    80\n",
      "                   Male      80\n",
      "poet               Female    80\n",
      "                   Male      80\n",
      "professor          Female    80\n",
      "                   Male      80\n",
      "psychologist       Female    80\n",
      "                   Male      80\n",
      "rapper             Female    80\n",
      "                   Male      80\n",
      "software_engineer  Female    80\n",
      "                   Male      80\n",
      "surgeon            Female    80\n",
      "                   Male      80\n",
      "teacher            Female    80\n",
      "                   Male      80\n",
      "yoga_teacher       Female    80\n",
      "                   Male      80\n",
      "dtype: int64\n",
      "\n",
      "Test set distribution:\n",
      "profession         gender\n",
      "accountant         Female    20\n",
      "                   Male      20\n",
      "architect          Female    20\n",
      "                   Male      20\n",
      "attorney           Female    20\n",
      "                   Male      20\n",
      "chiropractor       Female    20\n",
      "                   Male      20\n",
      "comedian           Female    20\n",
      "                   Male      20\n",
      "composer           Female    20\n",
      "                   Male      20\n",
      "dentist            Female    20\n",
      "                   Male      20\n",
      "dietitian          Female    20\n",
      "                   Male      20\n",
      "dj                 Female    20\n",
      "                   Male      20\n",
      "filmmaker          Female    20\n",
      "                   Male      20\n",
      "interior_designer  Female    20\n",
      "                   Male      20\n",
      "journalist         Female    20\n",
      "                   Male      20\n",
      "model              Female    20\n",
      "                   Male      20\n",
      "nurse              Female    20\n",
      "                   Male      20\n",
      "painter            Female    20\n",
      "                   Male      20\n",
      "paralegal          Female    20\n",
      "                   Male      20\n",
      "pastor             Female    20\n",
      "                   Male      20\n",
      "personal_trainer   Female    20\n",
      "                   Male      20\n",
      "photographer       Female    20\n",
      "                   Male      20\n",
      "physician          Female    20\n",
      "                   Male      20\n",
      "poet               Female    20\n",
      "                   Male      20\n",
      "professor          Female    20\n",
      "                   Male      20\n",
      "psychologist       Female    20\n",
      "                   Male      20\n",
      "rapper             Female    20\n",
      "                   Male      20\n",
      "software_engineer  Female    20\n",
      "                   Male      20\n",
      "surgeon            Female    20\n",
      "                   Male      20\n",
      "teacher            Female    20\n",
      "                   Male      20\n",
      "yoga_teacher       Female    20\n",
      "                   Male      20\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check gender and profession distribution in train and test sets\n",
    "print(\"Train set distribution:\")\n",
    "print(train_df.groupby(['profession', 'gender']).size())\n",
    "print(\"\\nTest set distribution:\")\n",
    "print(test_df.groupby(['profession', 'gender']).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d7ba1f2-cc07-40a6-b617-1dad9dd1337f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of occupations: 28\n"
     ]
    }
   ],
   "source": [
    "# Get all of the professions in the dataset\n",
    "occupations = train_df['profession'].unique().tolist()\n",
    "print(f\"Number of occupations: {len(occupations)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc27db1",
   "metadata": {},
   "source": [
    "---\n",
    "## Train model and measure bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cc1a2ad-5fef-4a27-ad00-3d94a46db5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at mideind/IceBERT and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Init model\n",
    "model, tokenizer, model_name = load_model_and_tokenizer(language,occupations)\n",
    "model.to(device)\n",
    "\n",
    "# Init discriminators\n",
    "n_discriminator = args.n_discriminator\n",
    "discriminators = [Discriminator(args, args.hidden_size, 2) for _ in range(n_discriminator)]\n",
    "discriminators = [dis.to(device) for dis in discriminators]\n",
    "\n",
    "# Init optimizers\n",
    "LEARNING_RATE = args.lr\n",
    "optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE)\n",
    "adv_optimizers = [Adam(filter(lambda p: p.requires_grad, dis.parameters()), lr=args.LAMBDA*LEARNING_RATE) for dis in discriminators]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1da9e22f-51bf-4ee1-8dad-53ce56fd5f71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_loader, test_loader = create_dataloaders(train_df, test_df, \u001b[43mtokenizer\u001b[49m, batch_size=params[\u001b[33m'\u001b[39m\u001b[33mbatch_size\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = create_dataloaders(train_df, test_df, tokenizer, batch_size=params['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cceeb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate the model\n",
    "if run_training and not pretrained:\n",
    "    trained_model, _, _ = train_and_evaluate(model, train_loader, test_loader, args, discriminators, optimizer, adv_optimizers, output_dir)\n",
    "else:\n",
    "    trained_model = model\n",
    "    if not run_training:\n",
    "        print(\"Skipping model training (run_training=False)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5649753e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to:\n",
      "  Full model: ../results/adv_debias/nb/bert_pretrained_model\n"
     ]
    }
   ],
   "source": [
    "# Save full model using Hugging Face format (for bias metrics)\n",
    "if run_training or pretrained:\n",
    "    trained_model.save_pretrained(output_model_dir)\n",
    "    tokenizer.save_pretrained(output_model_dir)\n",
    "\n",
    "    print(f\"Model saved to:\")\n",
    "    print(f\"  Full model: {output_model_dir}\")\n",
    "else:\n",
    "    print(\"Skipping model saving (run_training=False and not pretrained)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8adffd",
   "metadata": {},
   "source": [
    "### Bias in occupation predicton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f0b251-affb-43c8-b909-5b2b96a725f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_occupation_bias and not pretrained:\n",
    "    import gc\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Load the model WITHOUT specifying num_labels using conditional loading\n",
    "    print(f\"Loading model from {output_model_dir}\")\n",
    "    \n",
    "    # Determine model type based on language\n",
    "    current_model_type = model_types.get(language, \"bert\")  # default to BERT\n",
    "    \n",
    "    if current_model_type == \"roberta\":\n",
    "        trained_model = RobertaForSequenceClassification.from_pretrained(\n",
    "            output_model_dir, \n",
    "            output_hidden_states=True  # Don't specify num_labels!\n",
    "        )\n",
    "    else:\n",
    "        trained_model = BertForSequenceClassification.from_pretrained(\n",
    "            output_model_dir, \n",
    "            output_hidden_states=True  # Don't specify num_labels!\n",
    "        )\n",
    "    trained_model.to(device)\n",
    "    \n",
    "    # Print model info for debugging\n",
    "    print(f\"Loaded model config: {trained_model.config}\")\n",
    "    print(f\"Model num_labels: {trained_model.config.num_labels}\")\n",
    "    print(f\"Expected num_labels: {len(occupations)}\")\n",
    "    \n",
    "    # Measure accuracy for each gender overall and for each occupation\n",
    "    _, test_preds, test_labels, private_labels = eval_main(trained_model, test_loader, device, args)\n",
    "    \n",
    "    # Save results to a file\n",
    "    results = group_evaluation(test_preds, test_labels, private_labels, silence=True)\n",
    "    results_path = f\"{model_type}_{args.experiment_type}\"\n",
    "    if adversarial:\n",
    "        results_path += f\"_{num_discriminators}discriminators\"\n",
    "    if balance_sensitive:\n",
    "        results_path += f\"_balanced\"\n",
    "    if masked:\n",
    "        results_path += f\"_masked\"\n",
    "    results_file = os.path.join(output_dir, f\"{results_path}_results.json\")\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "elif not run_occupation_bias:\n",
    "    print(\"Skipping occupation prediction bias evaluation (run_occupation_bias=False)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4a3313",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_occupation_bias and not pretrained:\n",
    "    # Create enhanced confusion matrix plots with profession names\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Get profession names and class mapping\n",
    "    class_labels = results.get(\"class_labels\", list(range(len(occupations))))\n",
    "    profession_names = [occupations[i] if i < len(occupations) else f\"Class_{i}\" for i in class_labels]\n",
    "    \n",
    "    # Get confusion matrices\n",
    "    cm_0 = np.array(results[\"Group 0 confusion matrix\"])\n",
    "    cm_1 = np.array(results[\"Group 1 confusion matrix\"])\n",
    "    \n",
    "    # Create the plot with larger sizing for better label readability\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Plot Group 0 (typically female) confusion matrix\n",
    "    sns.heatmap(cm_0, \n",
    "               annot=True, \n",
    "               fmt='d', \n",
    "               cmap='Blues', \n",
    "               xticklabels=profession_names,\n",
    "               yticklabels=profession_names,\n",
    "               cbar=True,\n",
    "               square=True,\n",
    "               linewidths=0.5,\n",
    "               ax=axes[0])\n",
    "    axes[0].set_title('Group 0 (Female) Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "    axes[0].set_xlabel('Predicted Profession', fontsize=14)\n",
    "    axes[0].set_ylabel('Actual Profession', fontsize=14)\n",
    "    # Fix the rotation and font size for x-axis labels\n",
    "    axes[0].tick_params(axis='x', rotation=90, labelsize=10)\n",
    "    axes[0].tick_params(axis='y', rotation=0, labelsize=10)\n",
    "    \n",
    "    # Plot Group 1 (typically male) confusion matrix  \n",
    "    sns.heatmap(cm_1, \n",
    "               annot=True, \n",
    "               fmt='d', \n",
    "               cmap='Reds', \n",
    "               xticklabels=profession_names,\n",
    "               yticklabels=profession_names,\n",
    "               cbar=True,\n",
    "               square=True,\n",
    "               linewidths=0.5,\n",
    "               ax=axes[1])\n",
    "    axes[1].set_title('Group 1 (Male) Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "    axes[1].set_xlabel('Predicted Profession', fontsize=14)\n",
    "    axes[1].set_ylabel('Actual Profession', fontsize=14)\n",
    "    # Fix the rotation and font size for x-axis labels\n",
    "    axes[1].tick_params(axis='x', rotation=90, labelsize=10)\n",
    "    axes[1].tick_params(axis='y', rotation=0, labelsize=10)\n",
    "    \n",
    "    # Adjust layout to prevent label cutoff\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot with higher DPI for better quality\n",
    "    cm_file = os.path.join(output_dir, f\"{model_type}_{args.experiment_type}_confusion_matrices.png\")\n",
    "    plt.savefig(cm_file, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a44cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profession-level TPR Gap Analysis\n",
    "if run_occupation_bias and not pretrained:\n",
    "    from utils.eval_metrices import analyze_profession_tpr_gaps\n",
    "    from results_to_latex import create_profession_tpr_gap_table\n",
    "    \n",
    "    # Get profession names from the occupation list\n",
    "    profession_names = occupations\n",
    "    \n",
    "    # Run profession-level analysis\n",
    "    print(\"=== ANALYZING PROFESSION-LEVEL TPR GAPS ===\")\n",
    "    profession_tpr_analysis = analyze_profession_tpr_gaps(\n",
    "        test_preds, test_labels, private_labels, test_labels, profession_names, silence=False\n",
    "    )\n",
    "    \n",
    "    # Create LaTeX table\n",
    "    profession_tpr_table = create_profession_tpr_gap_table(profession_tpr_analysis, language=language)\n",
    "    \n",
    "    # Save the table\n",
    "    import os\n",
    "    table_file = os.path.join(output_dir, f\"{model_type}_{args.experiment_type}_profession_tpr_gaps.tex\")\n",
    "    with open(table_file, 'w') as f:\n",
    "        f.write(profession_tpr_table)\n",
    "    \n",
    "    # Also save as txt for easy viewing\n",
    "    table_txt_file = os.path.join(output_dir, f\"{model_type}_{args.experiment_type}_profession_tpr_gaps.txt\")\n",
    "    with open(table_txt_file, 'w') as f:\n",
    "        f.write(profession_tpr_table)\n",
    "    \n",
    "    print(f\"\\nProfession TPR gap table saved to:\")\n",
    "    print(f\"  LaTeX: {table_file}\")\n",
    "    print(f\"  Text: {table_txt_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c1f569",
   "metadata": {},
   "source": [
    "---\n",
    "### Bias in gender prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dcd16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ../results/adv_debias/nb/bert_pretrained_model\n",
      "Training hidden shape: (4480, 768), Private labels: 4480\n"
     ]
    }
   ],
   "source": [
    "if run_leakage_evaluation:\n",
    "    # Clear GPU memory more aggressively\n",
    "    import gc\n",
    "    import torch\n",
    "\n",
    "    # Delete any existing models\n",
    "    if 'trained_model' in locals():\n",
    "        del trained_model\n",
    "    if 'model' in locals():\n",
    "        del model\n",
    "    if 'discriminators' in locals():\n",
    "        del discriminators\n",
    "\n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "\n",
    "    # Clear PyTorch cache\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Reset PyTorch memory allocator (this helps with fragmentation)\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.reset_accumulated_memory_stats()\n",
    "\n",
    "    # Now load your model using conditional loading based on model type\n",
    "    print(f\"Loading model from {output_model_dir}\")\n",
    "\n",
    "    # Determine model type based on language\n",
    "    current_model_type = model_types.get(language, \"bert\")  # default to BERT\n",
    "\n",
    "    if current_model_type == \"roberta\":\n",
    "        trained_model = RobertaForSequenceClassification.from_pretrained(output_model_dir, num_labels=len(occupations), output_hidden_states=True)\n",
    "    else:\n",
    "        trained_model = BertForSequenceClassification.from_pretrained(output_model_dir, num_labels=len(occupations), output_hidden_states=True)\n",
    "    trained_model.to(device)\n",
    "\n",
    "    leakage_results = leakage_evaluation(trained_model, train_loader, test_loader, device)\n",
    "else:\n",
    "    print(\"Skipping leakage evaluation (run_leakage_evaluation=False)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49d762d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save leakage results to a file\n",
    "if run_leakage_evaluation:\n",
    "    leakage_results_root = f\"{model_type}_{args.experiment_type}\"\n",
    "\n",
    "    if balance_sensitive:\n",
    "        leakage_results_root += f\"_balanced\"\n",
    "    if masked:\n",
    "        leakage_results_root += f\"_masked\"\n",
    "    if adversarial:\n",
    "        leakage_results_root += f\"_{num_discriminators}discriminators\"\n",
    "\n",
    "    leakage_results_file = os.path.join(output_dir, f\"{leakage_results_root}_leakage_results.json\")\n",
    "\n",
    "    with open(leakage_results_file, 'w') as f:\n",
    "        json.dump(leakage_results, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad55d2e",
   "metadata": {},
   "source": [
    "---\n",
    "### Intrinsic Model Bias\n",
    "Here we run particular WEAT/SEAT/LPBS tests on the model.\n",
    "\n",
    "Tests:\n",
    "- male vs. female terms/surgeon vs. nurse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "930c68bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob # For finding test files\n",
    "\n",
    "def find_available_tests(language):\n",
    "    \"\"\"Find all available WEAT and LPBS tests for the given language.\"\"\"\n",
    "    \n",
    "    # Find WEAT tests\n",
    "    weat_path = f\"../data/wordlists/{language}/WEAT/\"\n",
    "    weat_tests = []\n",
    "    if os.path.exists(weat_path):\n",
    "        weat_files = glob.glob(os.path.join(weat_path, \"*.txt\"))\n",
    "        weat_tests = [os.path.splitext(os.path.basename(f))[0] for f in weat_files]\n",
    "        print(f\"Found {len(weat_tests)} WEAT tests: {weat_tests}\")\n",
    "    \n",
    "    # Find LPBS tests  \n",
    "    lpbs_path = f\"../data/wordlists/{language}/LPBS/\"\n",
    "    lpbs_tests = []\n",
    "    if os.path.exists(lpbs_path):\n",
    "        lpbs_files = glob.glob(os.path.join(lpbs_path, \"*.jsonl\"))\n",
    "        lpbs_tests = [os.path.splitext(os.path.basename(f))[0] for f in lpbs_files]\n",
    "        print(f\"Found {len(lpbs_tests)} LPBS tests: {lpbs_tests}\")\n",
    "    \n",
    "    return weat_tests, lpbs_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7ce765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pretrained model for intrinsic bias evaluation...\n",
      "Converting ROBERTAForSequenceClassification to ROBERTAForMaskedLM...\n",
      "Transferred: roberta.embeddings.word_embeddings.weight\n",
      "Transferred: roberta.embeddings.position_embeddings.weight\n",
      "Transferred: roberta.embeddings.token_type_embeddings.weight\n",
      "Transferred: roberta.embeddings.LayerNorm.weight\n",
      "Transferred: roberta.embeddings.LayerNorm.bias\n",
      "Transferred: roberta.encoder.layer.0.attention.self.query.weight\n",
      "Transferred: roberta.encoder.layer.0.attention.self.query.bias\n",
      "Transferred: roberta.encoder.layer.0.attention.self.key.weight\n",
      "Transferred: roberta.encoder.layer.0.attention.self.key.bias\n",
      "Transferred: roberta.encoder.layer.0.attention.self.value.weight\n",
      "Transferred: roberta.encoder.layer.0.attention.self.value.bias\n",
      "Transferred: roberta.encoder.layer.0.attention.output.dense.weight\n",
      "Transferred: roberta.encoder.layer.0.attention.output.dense.bias\n",
      "Transferred: roberta.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "Transferred: roberta.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "Transferred: roberta.encoder.layer.0.intermediate.dense.weight\n",
      "Transferred: roberta.encoder.layer.0.intermediate.dense.bias\n",
      "Transferred: roberta.encoder.layer.0.output.dense.weight\n",
      "Transferred: roberta.encoder.layer.0.output.dense.bias\n",
      "Transferred: roberta.encoder.layer.0.output.LayerNorm.weight\n",
      "Transferred: roberta.encoder.layer.0.output.LayerNorm.bias\n",
      "Transferred: roberta.encoder.layer.1.attention.self.query.weight\n",
      "Transferred: roberta.encoder.layer.1.attention.self.query.bias\n",
      "Transferred: roberta.encoder.layer.1.attention.self.key.weight\n",
      "Transferred: roberta.encoder.layer.1.attention.self.key.bias\n",
      "Transferred: roberta.encoder.layer.1.attention.self.value.weight\n",
      "Transferred: roberta.encoder.layer.1.attention.self.value.bias\n",
      "Transferred: roberta.encoder.layer.1.attention.output.dense.weight\n",
      "Transferred: roberta.encoder.layer.1.attention.output.dense.bias\n",
      "Transferred: roberta.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "Transferred: roberta.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "Transferred: roberta.encoder.layer.1.intermediate.dense.weight\n",
      "Transferred: roberta.encoder.layer.1.intermediate.dense.bias\n",
      "Transferred: roberta.encoder.layer.1.output.dense.weight\n",
      "Transferred: roberta.encoder.layer.1.output.dense.bias\n",
      "Transferred: roberta.encoder.layer.1.output.LayerNorm.weight\n",
      "Transferred: roberta.encoder.layer.1.output.LayerNorm.bias\n",
      "Transferred: roberta.encoder.layer.2.attention.self.query.weight\n",
      "Transferred: roberta.encoder.layer.2.attention.self.query.bias\n",
      "Transferred: roberta.encoder.layer.2.attention.self.key.weight\n",
      "Transferred: roberta.encoder.layer.2.attention.self.key.bias\n",
      "Transferred: roberta.encoder.layer.2.attention.self.value.weight\n",
      "Transferred: roberta.encoder.layer.2.attention.self.value.bias\n",
      "Transferred: roberta.encoder.layer.2.attention.output.dense.weight\n",
      "Transferred: roberta.encoder.layer.2.attention.output.dense.bias\n",
      "Transferred: roberta.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "Transferred: roberta.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "Transferred: roberta.encoder.layer.2.intermediate.dense.weight\n",
      "Transferred: roberta.encoder.layer.2.intermediate.dense.bias\n",
      "Transferred: roberta.encoder.layer.2.output.dense.weight\n",
      "Transferred: roberta.encoder.layer.2.output.dense.bias\n",
      "Transferred: roberta.encoder.layer.2.output.LayerNorm.weight\n",
      "Transferred: roberta.encoder.layer.2.output.LayerNorm.bias\n",
      "Transferred: roberta.encoder.layer.3.attention.self.query.weight\n",
      "Transferred: roberta.encoder.layer.3.attention.self.query.bias\n",
      "Transferred: roberta.encoder.layer.3.attention.self.key.weight\n",
      "Transferred: roberta.encoder.layer.3.attention.self.key.bias\n",
      "Transferred: roberta.encoder.layer.3.attention.self.value.weight\n",
      "Transferred: roberta.encoder.layer.3.attention.self.value.bias\n",
      "Transferred: roberta.encoder.layer.3.attention.output.dense.weight\n",
      "Transferred: roberta.encoder.layer.3.attention.output.dense.bias\n",
      "Transferred: roberta.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "Transferred: roberta.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "Transferred: roberta.encoder.layer.3.intermediate.dense.weight\n",
      "Transferred: roberta.encoder.layer.3.intermediate.dense.bias\n",
      "Transferred: roberta.encoder.layer.3.output.dense.weight\n",
      "Transferred: roberta.encoder.layer.3.output.dense.bias\n",
      "Transferred: roberta.encoder.layer.3.output.LayerNorm.weight\n",
      "Transferred: roberta.encoder.layer.3.output.LayerNorm.bias\n",
      "Transferred: roberta.encoder.layer.4.attention.self.query.weight\n",
      "Transferred: roberta.encoder.layer.4.attention.self.query.bias\n",
      "Transferred: roberta.encoder.layer.4.attention.self.key.weight\n",
      "Transferred: roberta.encoder.layer.4.attention.self.key.bias\n",
      "Transferred: roberta.encoder.layer.4.attention.self.value.weight\n",
      "Transferred: roberta.encoder.layer.4.attention.self.value.bias\n",
      "Transferred: roberta.encoder.layer.4.attention.output.dense.weight\n",
      "Transferred: roberta.encoder.layer.4.attention.output.dense.bias\n",
      "Transferred: roberta.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "Transferred: roberta.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "Transferred: roberta.encoder.layer.4.intermediate.dense.weight\n",
      "Transferred: roberta.encoder.layer.4.intermediate.dense.bias\n",
      "Transferred: roberta.encoder.layer.4.output.dense.weight\n",
      "Transferred: roberta.encoder.layer.4.output.dense.bias\n",
      "Transferred: roberta.encoder.layer.4.output.LayerNorm.weight\n",
      "Transferred: roberta.encoder.layer.4.output.LayerNorm.bias\n",
      "Transferred: roberta.encoder.layer.5.attention.self.query.weight\n",
      "Transferred: roberta.encoder.layer.5.attention.self.query.bias\n",
      "Transferred: roberta.encoder.layer.5.attention.self.key.weight\n",
      "Transferred: roberta.encoder.layer.5.attention.self.key.bias\n",
      "Transferred: roberta.encoder.layer.5.attention.self.value.weight\n",
      "Transferred: roberta.encoder.layer.5.attention.self.value.bias\n",
      "Transferred: roberta.encoder.layer.5.attention.output.dense.weight\n",
      "Transferred: roberta.encoder.layer.5.attention.output.dense.bias\n",
      "Transferred: roberta.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "Transferred: roberta.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "Transferred: roberta.encoder.layer.5.intermediate.dense.weight\n",
      "Transferred: roberta.encoder.layer.5.intermediate.dense.bias\n",
      "Transferred: roberta.encoder.layer.5.output.dense.weight\n",
      "Transferred: roberta.encoder.layer.5.output.dense.bias\n",
      "Transferred: roberta.encoder.layer.5.output.LayerNorm.weight\n",
      "Transferred: roberta.encoder.layer.5.output.LayerNorm.bias\n",
      "Transferred: roberta.encoder.layer.6.attention.self.query.weight\n",
      "Transferred: roberta.encoder.layer.6.attention.self.query.bias\n",
      "Transferred: roberta.encoder.layer.6.attention.self.key.weight\n",
      "Transferred: roberta.encoder.layer.6.attention.self.key.bias\n",
      "Transferred: roberta.encoder.layer.6.attention.self.value.weight\n",
      "Transferred: roberta.encoder.layer.6.attention.self.value.bias\n",
      "Transferred: roberta.encoder.layer.6.attention.output.dense.weight\n",
      "Transferred: roberta.encoder.layer.6.attention.output.dense.bias\n",
      "Transferred: roberta.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "Transferred: roberta.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "Transferred: roberta.encoder.layer.6.intermediate.dense.weight\n",
      "Transferred: roberta.encoder.layer.6.intermediate.dense.bias\n",
      "Transferred: roberta.encoder.layer.6.output.dense.weight\n",
      "Transferred: roberta.encoder.layer.6.output.dense.bias\n",
      "Transferred: roberta.encoder.layer.6.output.LayerNorm.weight\n",
      "Transferred: roberta.encoder.layer.6.output.LayerNorm.bias\n",
      "Transferred: roberta.encoder.layer.7.attention.self.query.weight\n",
      "Transferred: roberta.encoder.layer.7.attention.self.query.bias\n",
      "Transferred: roberta.encoder.layer.7.attention.self.key.weight\n",
      "Transferred: roberta.encoder.layer.7.attention.self.key.bias\n",
      "Transferred: roberta.encoder.layer.7.attention.self.value.weight\n",
      "Transferred: roberta.encoder.layer.7.attention.self.value.bias\n",
      "Transferred: roberta.encoder.layer.7.attention.output.dense.weight\n",
      "Transferred: roberta.encoder.layer.7.attention.output.dense.bias\n",
      "Transferred: roberta.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "Transferred: roberta.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "Transferred: roberta.encoder.layer.7.intermediate.dense.weight\n",
      "Transferred: roberta.encoder.layer.7.intermediate.dense.bias\n",
      "Transferred: roberta.encoder.layer.7.output.dense.weight\n",
      "Transferred: roberta.encoder.layer.7.output.dense.bias\n",
      "Transferred: roberta.encoder.layer.7.output.LayerNorm.weight\n",
      "Transferred: roberta.encoder.layer.7.output.LayerNorm.bias\n",
      "Transferred: roberta.encoder.layer.8.attention.self.query.weight\n",
      "Transferred: roberta.encoder.layer.8.attention.self.query.bias\n",
      "Transferred: roberta.encoder.layer.8.attention.self.key.weight\n",
      "Transferred: roberta.encoder.layer.8.attention.self.key.bias\n",
      "Transferred: roberta.encoder.layer.8.attention.self.value.weight\n",
      "Transferred: roberta.encoder.layer.8.attention.self.value.bias\n",
      "Transferred: roberta.encoder.layer.8.attention.output.dense.weight\n",
      "Transferred: roberta.encoder.layer.8.attention.output.dense.bias\n",
      "Transferred: roberta.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "Transferred: roberta.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "Transferred: roberta.encoder.layer.8.intermediate.dense.weight\n",
      "Transferred: roberta.encoder.layer.8.intermediate.dense.bias\n",
      "Transferred: roberta.encoder.layer.8.output.dense.weight\n",
      "Transferred: roberta.encoder.layer.8.output.dense.bias\n",
      "Transferred: roberta.encoder.layer.8.output.LayerNorm.weight\n",
      "Transferred: roberta.encoder.layer.8.output.LayerNorm.bias\n",
      "Transferred: roberta.encoder.layer.9.attention.self.query.weight\n",
      "Transferred: roberta.encoder.layer.9.attention.self.query.bias\n",
      "Transferred: roberta.encoder.layer.9.attention.self.key.weight\n",
      "Transferred: roberta.encoder.layer.9.attention.self.key.bias\n",
      "Transferred: roberta.encoder.layer.9.attention.self.value.weight\n",
      "Transferred: roberta.encoder.layer.9.attention.self.value.bias\n",
      "Transferred: roberta.encoder.layer.9.attention.output.dense.weight\n",
      "Transferred: roberta.encoder.layer.9.attention.output.dense.bias\n",
      "Transferred: roberta.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "Transferred: roberta.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "Transferred: roberta.encoder.layer.9.intermediate.dense.weight\n",
      "Transferred: roberta.encoder.layer.9.intermediate.dense.bias\n",
      "Transferred: roberta.encoder.layer.9.output.dense.weight\n",
      "Transferred: roberta.encoder.layer.9.output.dense.bias\n",
      "Transferred: roberta.encoder.layer.9.output.LayerNorm.weight\n",
      "Transferred: roberta.encoder.layer.9.output.LayerNorm.bias\n",
      "Transferred: roberta.encoder.layer.10.attention.self.query.weight\n",
      "Transferred: roberta.encoder.layer.10.attention.self.query.bias\n",
      "Transferred: roberta.encoder.layer.10.attention.self.key.weight\n",
      "Transferred: roberta.encoder.layer.10.attention.self.key.bias\n",
      "Transferred: roberta.encoder.layer.10.attention.self.value.weight\n",
      "Transferred: roberta.encoder.layer.10.attention.self.value.bias\n",
      "Transferred: roberta.encoder.layer.10.attention.output.dense.weight\n",
      "Transferred: roberta.encoder.layer.10.attention.output.dense.bias\n",
      "Transferred: roberta.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "Transferred: roberta.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "Transferred: roberta.encoder.layer.10.intermediate.dense.weight\n",
      "Transferred: roberta.encoder.layer.10.intermediate.dense.bias\n",
      "Transferred: roberta.encoder.layer.10.output.dense.weight\n",
      "Transferred: roberta.encoder.layer.10.output.dense.bias\n",
      "Transferred: roberta.encoder.layer.10.output.LayerNorm.weight\n",
      "Transferred: roberta.encoder.layer.10.output.LayerNorm.bias\n",
      "Transferred: roberta.encoder.layer.11.attention.self.query.weight\n",
      "Transferred: roberta.encoder.layer.11.attention.self.query.bias\n",
      "Transferred: roberta.encoder.layer.11.attention.self.key.weight\n",
      "Transferred: roberta.encoder.layer.11.attention.self.key.bias\n",
      "Transferred: roberta.encoder.layer.11.attention.self.value.weight\n",
      "Transferred: roberta.encoder.layer.11.attention.self.value.bias\n",
      "Transferred: roberta.encoder.layer.11.attention.output.dense.weight\n",
      "Transferred: roberta.encoder.layer.11.attention.output.dense.bias\n",
      "Transferred: roberta.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "Transferred: roberta.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "Transferred: roberta.encoder.layer.11.intermediate.dense.weight\n",
      "Transferred: roberta.encoder.layer.11.intermediate.dense.bias\n",
      "Transferred: roberta.encoder.layer.11.output.dense.weight\n",
      "Transferred: roberta.encoder.layer.11.output.dense.bias\n",
      "Transferred: roberta.encoder.layer.11.output.LayerNorm.weight\n",
      "Transferred: roberta.encoder.layer.11.output.LayerNorm.bias\n",
      "MLM model saved to: ../results/adv_debias/is/temp_roberta_pretrained_model_mlm\n",
      "Using converted MLM model at: ../results/adv_debias/is/temp_roberta_pretrained_model_mlm\n",
      "Model exists: True\n",
      "Found 21 WEAT tests: ['GER1', 'GER2', 'IS8_v2', 'IS6_v2', 'IS3', 'IS2', 'WEAT_7', 'WEAT_6', 'IS1', 'IS5', 'IS4', 'IS6', 'WEAT_7_v2', 'GER1_v2', 'WEAT_7_v3', 'IS7', 'IS8', 'IS7_v2', 'WEAT_8', 'GER2_v2', 'WEAT_8_v2']\n",
      "Found 21 LPBS tests: ['LPBS_GER1', 'LPBS_WEAT_7_v3', 'LPBS_WEAT_6', 'LPBS_WEAT_7', 'LPBS_IS8', 'LPBS_WEAT_7_v2', 'LPBS_GER2_v2', 'LPBS_GER2', 'LPBS_IS7_v2', 'LPBS_IS6_v2', 'LPBS_IS6', 'LPBS_IS4', 'LPBS_WEAT_8_v2', 'LPBS_IS8_v2', 'LPBS_IS2', 'LPBS_IS5', 'LPBS_IS7', 'LPBS_WEAT_8', 'LPBS_GER1_v2', 'LPBS_IS3', 'LPBS_IS1']\n",
      "\n",
      "Total combinations to evaluate: 42\n",
      "Test combinations:\n",
      "  1. GER1 - WEAT - bert\n",
      "  2. GER2 - WEAT - bert\n",
      "  3. IS8_v2 - WEAT - bert\n",
      "  4. IS6_v2 - WEAT - bert\n",
      "  5. IS3 - WEAT - bert\n",
      "  6. IS2 - WEAT - bert\n",
      "  7. WEAT_7 - WEAT - bert\n",
      "  8. WEAT_6 - WEAT - bert\n",
      "  9. IS1 - WEAT - bert\n",
      "  10. IS5 - WEAT - bert\n",
      "  11. IS4 - WEAT - bert\n",
      "  12. IS6 - WEAT - bert\n",
      "  13. WEAT_7_v2 - WEAT - bert\n",
      "  14. GER1_v2 - WEAT - bert\n",
      "  15. WEAT_7_v3 - WEAT - bert\n",
      "  16. IS7 - WEAT - bert\n",
      "  17. IS8 - WEAT - bert\n",
      "  18. IS7_v2 - WEAT - bert\n",
      "  19. WEAT_8 - WEAT - bert\n",
      "  20. GER2_v2 - WEAT - bert\n",
      "  21. WEAT_8_v2 - WEAT - bert\n",
      "  22. LPBS_GER1 - LPBS - bert\n",
      "  23. LPBS_WEAT_7_v3 - LPBS - bert\n",
      "  24. LPBS_WEAT_6 - LPBS - bert\n",
      "  25. LPBS_WEAT_7 - LPBS - bert\n",
      "  26. LPBS_IS8 - LPBS - bert\n",
      "  27. LPBS_WEAT_7_v2 - LPBS - bert\n",
      "  28. LPBS_GER2_v2 - LPBS - bert\n",
      "  29. LPBS_GER2 - LPBS - bert\n",
      "  30. LPBS_IS7_v2 - LPBS - bert\n",
      "  31. LPBS_IS6_v2 - LPBS - bert\n",
      "  32. LPBS_IS6 - LPBS - bert\n",
      "  33. LPBS_IS4 - LPBS - bert\n",
      "  34. LPBS_WEAT_8_v2 - LPBS - bert\n",
      "  35. LPBS_IS8_v2 - LPBS - bert\n",
      "  36. LPBS_IS2 - LPBS - bert\n",
      "  37. LPBS_IS5 - LPBS - bert\n",
      "  38. LPBS_IS7 - LPBS - bert\n",
      "  39. LPBS_WEAT_8 - LPBS - bert\n",
      "  40. LPBS_GER1_v2 - LPBS - bert\n",
      "  41. LPBS_IS3 - LPBS - bert\n",
      "  42. LPBS_IS1 - LPBS - bert\n",
      "\n",
      "Evaluating intrinsic bias on model (converted to MLM)...\n",
      "\n",
      "Evaluating GER1 with WEAT and bert for language is with model ../results/adv_debias/is/temp_roberta_pretrained_model_mlm...\n",
      "\n",
      "Using bert model from cache at ../results/adv_debias/is/temp_roberta_pretrained_model_mlm\n",
      "Model device: cpu\n",
      "effect size: 0.37\n",
      "calculating p-value, % of permutations done: \n",
      "10.0%, 20.0%, 30.0%, 40.0%, 50.0%, 60.0%, 70.0%, 80.0%, 90.0%, 100.0%, \n",
      "\n",
      "p-value:  0.3004\n",
      "✓ Successfully evaluated GER1 with WEAT\n",
      "\n",
      "Evaluating GER2 with WEAT and bert for language is with model ../results/adv_debias/is/temp_roberta_pretrained_model_mlm...\n",
      "\n",
      "Using bert model from cache at ../results/adv_debias/is/temp_roberta_pretrained_model_mlm\n",
      "Model device: cpu\n",
      "effect size: 0.03\n",
      "calculating p-value, % of permutations done: \n",
      "10.0%, 20.0%, 30.0%, 40.0%, 50.0%, 60.0%, 70.0%, 80.0%, 90.0%, 100.0%, \n",
      "\n",
      "p-value:  0.4856\n",
      "✓ Successfully evaluated GER2 with WEAT\n",
      "\n",
      "Evaluating IS8_v2 with WEAT and bert for language is with model ../results/adv_debias/is/temp_roberta_pretrained_model_mlm...\n",
      "\n",
      "Using bert model from cache at ../results/adv_debias/is/temp_roberta_pretrained_model_mlm\n",
      "Model device: cpu\n",
      "effect size: -1.17\n",
      "calculating p-value, % of permutations done: \n",
      "10.0%, 20.0%, 30.0%, 40.0%, 50.0%, 60.0%, 70.0%, 80.0%, 90.0%, 100.0%, \n",
      "\n",
      "p-value:  0.8256\n",
      "✓ Successfully evaluated IS8_v2 with WEAT\n",
      "\n",
      "Evaluating IS6_v2 with WEAT and bert for language is with model ../results/adv_debias/is/temp_roberta_pretrained_model_mlm...\n",
      "\n",
      "Using bert model from cache at ../results/adv_debias/is/temp_roberta_pretrained_model_mlm\n",
      "Model device: cpu\n",
      "effect size: -0.44\n",
      "calculating p-value, % of permutations done: \n",
      "10.0%, 20.0%, 30.0%, 40.0%, 50.0%, 60.0%, 70.0%, 80.0%, 90.0%, 100.0%, \n",
      "\n",
      "p-value:  0.5806\n",
      "✓ Successfully evaluated IS6_v2 with WEAT\n",
      "\n",
      "Evaluating IS3 with WEAT and bert for language is with model ../results/adv_debias/is/temp_roberta_pretrained_model_mlm...\n",
      "\n",
      "Using bert model from cache at ../results/adv_debias/is/temp_roberta_pretrained_model_mlm\n",
      "Model device: cpu\n",
      "effect size: 0.27\n",
      "calculating p-value, % of permutations done: \n",
      "10.0%, 20.0%, 30.0%, 40.0%, 50.0%, "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 98\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEvaluating intrinsic bias on model (converted to MLM)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     96\u001b[39m \u001b[38;5;66;03m# The evaluate_combinations function now handles individual test failures internally\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# and returns partial results instead of failing completely\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m intrinsic_results = \u001b[43mevaluate_combinations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombinations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfullpermut\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mIntrinsic bias evaluation completed!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m intrinsic_results:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/BFH/github_repos/bias_mitigation_BERT_multilingual/src/bias_metrics/utils.py:156\u001b[39m, in \u001b[36mevaluate_combinations\u001b[39m\u001b[34m(combinations, calc_pvalue_bool, p_value_iterations, full_permut_bool)\u001b[39m\n\u001b[32m    153\u001b[39m dataset_obj[\u001b[33m\"\u001b[39m\u001b[33mAttribute2\u001b[39m\u001b[33m\"\u001b[39m] = filtered_lines[\u001b[32m3\u001b[39m].split(\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    154\u001b[39m \u001b[38;5;66;03m#print(\"Dataset_Obj:\",dataset_obj)\u001b[39;00m\n\u001b[32m    155\u001b[39m \u001b[38;5;66;03m# Evaluate and store the result\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m result = \u001b[43mweat_tester\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcalc_pvalue_bool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_value_iterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_permut_bool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m modelname:\n\u001b[32m    158\u001b[39m     results[(dataset, metric, embedding, language, modelname)] = result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/BFH/github_repos/bias_mitigation_BERT_multilingual/src/bias_metrics/metrics.py:119\u001b[39m, in \u001b[36mWEAT.evaluate\u001b[39m\u001b[34m(self, dataset, calc_pval, number_permut, full_permut)\u001b[39m\n\u001b[32m    117\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mFull permutation test is currently not implemented.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m         p_value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpermutation_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_permutations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnumber_permut\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mp-value: \u001b[39m\u001b[33m\"\u001b[39m, p_value)\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    124\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mEffect Size\u001b[39m\u001b[33m\"\u001b[39m: effect,\n\u001b[32m    125\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mp-value\u001b[39m\u001b[33m\"\u001b[39m: p_value\n\u001b[32m    126\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/BFH/github_repos/bias_mitigation_BERT_multilingual/src/bias_metrics/metrics.py:91\u001b[39m, in \u001b[36mWEAT.permutation_test\u001b[39m\u001b[34m(self, embeddings, num_permutations)\u001b[39m\n\u001b[32m     88\u001b[39m new_target1 = combined[:size]\n\u001b[32m     89\u001b[39m new_target2 = combined[size:]\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m test_stat = \u001b[43meffect_size_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_target1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_target2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattribute1_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattribute2_embed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m test_stat > observed_test_stat:\n\u001b[32m     93\u001b[39m     count += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/BFH/github_repos/bias_mitigation_BERT_multilingual/src/bias_metrics/metric_helper_functions.py:28\u001b[39m, in \u001b[36meffect_size_embed\u001b[39m\u001b[34m(target1_embed, target2_embed, attribute1_embed, attribute2_embed)\u001b[39m\n\u001b[32m     26\u001b[39m mean_target1 = np.mean([s(t, attribute1_embed, attribute2_embed) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m target1_embed]) \u001b[38;5;66;03m# asspciation of all target1 with attribute1 and attribute2\u001b[39;00m\n\u001b[32m     27\u001b[39m mean_target2 = np.mean([s(t, attribute1_embed, attribute2_embed) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m target2_embed])\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m stdev = np.std([\u001b[43ms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattribute1_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattribute2_embed\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m target1_embed + target2_embed]) \u001b[38;5;66;03m#std over all: target1 and target2\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (mean_target1 - mean_target2) / stdev\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/BFH/github_repos/bias_mitigation_BERT_multilingual/src/bias_metrics/metric_helper_functions.py:19\u001b[39m, in \u001b[36ms\u001b[39m\u001b[34m(w_embed, A_embed, B_embed)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34ms\u001b[39m(w_embed, A_embed, B_embed):\n\u001b[32m     16\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[33;03m    Measurement of association between one target word and two attribute words\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.mean([cosine_similarity(w_embed, a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m A_embed]) - np.mean([\u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m B_embed])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/BFH/github_repos/bias_mitigation_BERT_multilingual/src/bias_metrics/metric_helper_functions.py:13\u001b[39m, in \u001b[36mcosine_similarity\u001b[39m\u001b[34m(v1, v2)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcosine_similarity\u001b[39m(v1, v2):\n\u001b[32m     10\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[33;03m    Cosine similarity between two vectors.\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.dot(v1, v2) / (np.linalg.norm(v1) * \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv2\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/numpy/linalg/_linalg.py:2785\u001b[39m, in \u001b[36mnorm\u001b[39m\u001b[34m(x, ord, axis, keepdims)\u001b[39m\n\u001b[32m   2779\u001b[39m ndim = x.ndim\n\u001b[32m   2780\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2781\u001b[39m     (\u001b[38;5;28mord\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m   2782\u001b[39m     (\u001b[38;5;28mord\u001b[39m \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m'\u001b[39m\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfro\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m ndim == \u001b[32m2\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m   2783\u001b[39m     (\u001b[38;5;28mord\u001b[39m == \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m ndim == \u001b[32m1\u001b[39m)\n\u001b[32m   2784\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m2785\u001b[39m     x = \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mK\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2786\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m isComplexType(x.dtype.type):\n\u001b[32m   2787\u001b[39m         x_real = x.real\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from bias_metrics.utils import evaluate_combinations\n",
    "from scripts_adv_debias import convert_classification_to_mlm\n",
    "import os\n",
    "\n",
    "### Run intrinsic bias tests ###\n",
    "if run_intrinsic == True:\n",
    "    \n",
    "    ### Retrieve directory of current model ###\n",
    "    output_model_dir = os.path.join(output_dir, f\"{model_type}_{args.experiment_type}_model\")\n",
    "\n",
    "    if not pretrained:\n",
    "        # If adversarial training was used, add the number of discriminators to the output directory name\n",
    "        if adversarial:\n",
    "            output_model_dir += f\"_{num_discriminators}discriminators\"\n",
    "        \n",
    "        # If balanced dataset, add to output directory name\n",
    "        if balance_sensitive:\n",
    "            output_model_dir += f\"_balanced\"\n",
    "        \n",
    "        # If masking, add to output directory name\n",
    "        if masked:\n",
    "            output_model_dir += f\"_masked\"\n",
    "            \n",
    "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Load the trained model using conditional loading based on model type\n",
    "        print(f'Loading saved model from: {output_model_dir}...')\n",
    "        \n",
    "        # Determine model type based on language\n",
    "        current_model_type = model_types.get(language, \"bert\")  # default to BERT\n",
    "        \n",
    "        if current_model_type == \"roberta\":\n",
    "            trained_model = RobertaForSequenceClassification.from_pretrained(output_model_dir, num_labels=len(occupations), output_hidden_states=True)\n",
    "        else:\n",
    "            trained_model = BertForSequenceClassification.from_pretrained(output_model_dir, num_labels=len(occupations), output_hidden_states=True)\n",
    "        trained_model.to(device)\n",
    "    else:\n",
    "        # For pretrained models, use the already loaded model\n",
    "        print(\"Using pretrained model for intrinsic bias evaluation...\")\n",
    "        trained_model = model  # Use the model already loaded above\n",
    "    \n",
    "    # Experiment configurations\n",
    "    pval = True\n",
    "    iterations = 10000\n",
    "    fullpermut = False\n",
    "    \n",
    "    # Convert your fine-tuned model to MLM format with model_type parameter\n",
    "    current_model_type = model_types.get(language, \"bert\")  # default to BERT\n",
    "    print(f\"Converting {current_model_type.upper()}ForSequenceClassification to {current_model_type.upper()}ForMaskedLM...\")\n",
    "    mlm_model = convert_classification_to_mlm(trained_model, model_name, current_model_type)\n",
    "    \n",
    "    # Create MLM model directory with same name as classifier but with _mlm suffix\n",
    "    if pretrained:\n",
    "        # For pretrained models, create a temporary directory\n",
    "        mlm_output_dir = os.path.join(output_dir, f\"temp_{model_type}_{args.experiment_type}_model_mlm\")\n",
    "    else:\n",
    "        mlm_output_dir = output_model_dir + \"_mlm\"\n",
    "    \n",
    "    Path(mlm_output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    mlm_model.save_pretrained(mlm_output_dir)\n",
    "    tokenizer.save_pretrained(mlm_output_dir)\n",
    "    \n",
    "    print(f\"MLM model saved to: {mlm_output_dir}\")\n",
    "    if not pretrained:\n",
    "        print(f\"Original classifier preserved at: {output_model_dir}\")\n",
    "    \n",
    "    # Use the converted MLM model\n",
    "    mlm_model_path = mlm_output_dir\n",
    "    \n",
    "    print(f\"Using converted MLM model at: {mlm_model_path}\")\n",
    "    print(f\"Model exists: {os.path.exists(mlm_model_path)}\")\n",
    "    \n",
    "    # Find all available tests for the current language\n",
    "    weat_tests, lpbs_tests = find_available_tests(language)\n",
    "    \n",
    "    # Create combinations for all available tests\n",
    "    combinations = []\n",
    "    \n",
    "    # Add WEAT tests with different embedding types\n",
    "    for test in weat_tests:\n",
    "        combinations.append((test, 'WEAT', 'bert', language, mlm_model_path))\n",
    "    \n",
    "    # Add LPBS tests\n",
    "    for test in lpbs_tests:\n",
    "        combinations.append((test, 'LPBS', 'bert', language, mlm_model_path))\n",
    "    \n",
    "    print(f\"\\nTotal combinations to evaluate: {len(combinations)}\")\n",
    "    print(\"Test combinations:\")\n",
    "    for i, combo in enumerate(combinations, 1):\n",
    "        print(f\"  {i}. {combo[0]} - {combo[1]} - {combo[2]}\")\n",
    "    \n",
    "    if combinations:\n",
    "        print(\"\\nEvaluating intrinsic bias on model (converted to MLM)...\")\n",
    "        \n",
    "        # The evaluate_combinations function now handles individual test failures internally\n",
    "        # and returns partial results instead of failing completely\n",
    "        intrinsic_results = evaluate_combinations(combinations, pval, iterations, fullpermut)\n",
    "        \n",
    "        print(\"\\nIntrinsic bias evaluation completed!\")\n",
    "        if intrinsic_results:\n",
    "            print(f\"Results for {len(intrinsic_results)} test combination(s):\")\n",
    "            \n",
    "            # Print summary of results\n",
    "            for metric, result in intrinsic_results.items():\n",
    "                if isinstance(result, dict) and 'Effect Size' in result:\n",
    "                    print(f\"  {metric}: Effect Size = {result['Effect Size']:.4f}\")\n",
    "                else:\n",
    "                    print(f\"  {metric}: {result}\")\n",
    "        else:\n",
    "            print(\"⚠️  WARNING: No successful test results obtained!\")\n",
    "        \n",
    "        # Save intrinsic results as a text file (even if partial or empty)\n",
    "        intrinsic_root = f\"{model_type}_{args.experiment_type}_intrinsic_results\"\n",
    "        if not pretrained:\n",
    "            if adversarial:\n",
    "                intrinsic_root += f\"_{num_discriminators}discriminators\"\n",
    "            \n",
    "            # If balanced dataset, add to output directory name\n",
    "            if balance_sensitive:\n",
    "                intrinsic_root += f\"_balanced\"\n",
    "\n",
    "            # If masking, add to output directory name\n",
    "            if masked:\n",
    "                intrinsic_root += f\"_masked\"\n",
    "        \n",
    "        intrinsic_results_file = os.path.join(output_dir, intrinsic_root+\".txt\")\n",
    "        with open(intrinsic_results_file, 'w') as f:\n",
    "            if intrinsic_results:\n",
    "                for metric, result in intrinsic_results.items():\n",
    "                    f.write(f\"{metric}: {result}\\n\")\n",
    "            else:\n",
    "                f.write(\"No successful test results obtained. All tests failed.\\n\")\n",
    "                f.write(f\"Total tests attempted: {len(combinations)}\\n\")\n",
    "        \n",
    "        print(f\"\\nResults saved to: {intrinsic_results_file}\")\n",
    "        \n",
    "        # Clean up temporary MLM model directory if we created one for pretrained models\n",
    "        if pretrained and os.path.exists(mlm_output_dir):\n",
    "            import shutil\n",
    "            shutil.rmtree(mlm_output_dir)\n",
    "            print(f\"Cleaned up temporary MLM model directory: {mlm_output_dir}\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"No WEAT or LPBS tests found for language '{language}'\")\n",
    "        print(f\"Please check that test files exist in:\")\n",
    "        print(f\"  - ../data/wordlists/{language}/WEAT/\")\n",
    "        print(f\"  - ../data/wordlists/{language}/LPBS/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636160d7-3805-4c47-a8e5-ed4194869c86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
